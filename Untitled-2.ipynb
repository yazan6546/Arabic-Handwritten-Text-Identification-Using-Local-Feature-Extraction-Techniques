{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Transformation Pipeline\n",
    "\n",
    "This notebook demonstrates the process of feature extraction, clustering, histogram creation, and IDF computation using custom transformers and pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from classes.feature_extractor import FeatureExtractor\n",
    "from classes.clusterer import Clusterer\n",
    "from classes.idf_transformer import IDFTransformer\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load images, create a DataFrame, encode labels, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from a directory\n",
    "def load_images_from_directory(directory):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                img = cv2.imread(os.path.join(root, filename), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    data.append({'filename': filename, 'image': img})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing images\n",
    "image_directory = 'data/preprocessed'\n",
    "\n",
    "# Load images\n",
    "images_data = load_images_from_directory(image_directory)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(images_data)\n",
    "df.set_index('filename', inplace=True)\n",
    "df['Target'] = df.index.map(lambda x: x.split('_')[0])\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Target'] = label_encoder.fit_transform(df['Target'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Pipelines\n",
    "\n",
    "Create separate pipelines for ORB and SIFT that include all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract images from DataFrame\n",
    "def extract_images(df):\n",
    "    if 'image' not in df.columns:\n",
    "        raise KeyError(\"The DataFrame does not contain an 'image' column.\")\n",
    "    return df['image']\n",
    "\n",
    "# Create the pipeline for ORB\n",
    "pipeline_ORB = Pipeline([\n",
    "    ('extract_images', FunctionTransformer(extract_images, validate=False)),\n",
    "    ('feature_extractor', FeatureExtractor(method='ORB')),\n",
    "    ('clusterer', Clusterer(num_clusters=700)),\n",
    "    # ('idf_transformer', IDFTransformer())\n",
    "])\n",
    "\n",
    "# Create the pipeline for SIFT\n",
    "pipeline_SIFT = Pipeline([\n",
    "    ('extract_images', FunctionTransformer(extract_images, validate=False)),\n",
    "    ('feature_extractor', FeatureExtractor(method='SIFT')),\n",
    "    ('clusterer', Clusterer(num_clusters=700)),\n",
    "    # ('idf_transformer', IDFTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal kmeans clusters number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster number: 100, Inertia: 109098601195.24123, Time: 269.60 seconds\n",
      "Cluster number: 200, Inertia: 96520972733.90935, Time: 456.07 seconds\n",
      "Cluster number: 300, Inertia: 89994675774.00465, Time: 707.79 seconds\n",
      "Cluster number: 400, Inertia: 85385920715.80511, Time: 916.88 seconds\n"
     ]
    }
   ],
   "source": [
    "Clusterer.find_best_cluster_number(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data and Add Histograms to DataFrame\n",
    "\n",
    "Use the pipelines to fit and transform the training data, and add the resulting histograms back into the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipelines on the training data\n",
    "orb_histograms_train = pipeline_ORB.fit_transform(train_df)\n",
    "sift_histograms_train = pipeline_SIFT.fit_transform(train_df)\n",
    "\n",
    "# Transform the testing data\n",
    "orb_histograms_test = pipeline_ORB.transform(test_df)\n",
    "sift_histograms_test = pipeline_SIFT.transform(test_df)\n",
    "\n",
    "# Add histograms to DataFrame\n",
    "train_df['histogram_ORB_IDF'] = list(orb_histograms_train)\n",
    "test_df['histogram_ORB_IDF'] = list(orb_histograms_test)\n",
    "train_df['histogram_SIFT_IDF'] = list(sift_histograms_train)\n",
    "test_df['histogram_SIFT_IDF'] = list(sift_histograms_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(orb_histograms_train, 'models/pipeline_ORB_100.joblib')\n",
    "joblib.dump(sift_histograms_train, 'models/pipeline_SIFT_100.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_histograms=joblib.load('models/orb_histograms.joblib')\n",
    "sift_histograms=joblib.load('models/sift_histograms.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first histogram to verify\n",
    "if not train_df.empty:\n",
    "    first_user = train_df.index[1]\n",
    "    first_histogram = train_df.iloc[1]['histogram_ORB_IDF']\n",
    "    plt.bar(range(100), first_histogram)\n",
    "    plt.title(f'Visual Words Histogram for User: {first_user}')\n",
    "    plt.xlabel('Visual Word Index')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Create Pipelines\n",
    "# -----------------------------\n",
    "# Pipeline for ORB Features with SVM Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "pipeline_ORB_SVM = Pipeline([\n",
    "    ('extract_images', FunctionTransformer(extract_images, validate=False)),\n",
    "    ('feature_extractor', FeatureExtractor(method='ORB')),\n",
    "    ('clusterer', Clusterer(num_clusters=100)),\n",
    "    # ('idf_transformer', IDFTransformer()),\n",
    "    ('classifier', SVC(kernel='rbf', C=50, random_state=0))\n",
    "])\n",
    "\n",
    "# Pipeline for SIFT Features with SVM Classifier\n",
    "pipeline_SIFT_SVM = Pipeline([\n",
    "    ('extract_images', FunctionTransformer(extract_images, validate=False)),\n",
    "    ('feature_extractor', FeatureExtractor(method='SIFT')),\n",
    "    ('clusterer', Clusterer(num_clusters=100)),\n",
    "    # ('idf_transformer', IDFTransformer()),\n",
    "    ('classifier', SVC(kernel='rbf', C=50, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipelines on the training data\n",
    "pipeline_ORB_SVM.fit(train_df, train_df['Target'])\n",
    "# pipeline_SIFT_SVM.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeline_ORB_SVM, 'models/pipeline_ORB_SVM_100.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ORB_SVM=joblib.load('models/pipeline_ORB_SVM_100.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pipeline_ORB_SVM.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure all target labels are of type np.int64\n",
    "train_df['Target'] = train_df['Target'].astype(np.int64)\n",
    "test_df['Target'] = test_df['Target'].astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['Target'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test data\n",
    "\n",
    "# Make predictions on the test set\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "print(\"Making predictions on the test set...\")\n",
    "predictions = pipeline_ORB_SVM.predict(test_df)\n",
    "print(\"Predictions completed.\\n\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(test_df['Target'], predictions)\n",
    "print(f\"Accuracy of the pipeline: {accuracy:.2f}\")\n",
    "\n",
    "# # # Generate and print the classification report\n",
    "# report = classification_report(test_df['Target'], predictions, target_names=label_encoder.classes_)\n",
    "# print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in list(test_df['Target'].values):\n",
    "    if type(x) is not np.int64:\n",
    "        print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Define Parameter Grid for GridSearchCV\n",
    "# -----------------------------\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'clusterer__num_clusters': [200, 300, 400, 500, 600, 700, 800]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize GridSearchCV for ORB Pipeline\n",
    "# -----------------------------\n",
    "grid_search_ORB = GridSearchCV(\n",
    "    estimator=pipeline_ORB_SVM,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Fit GridSearchCV for ORB Pipeline\n",
    "# -----------------------------\n",
    "from sklearn.metrics import classification_report, accuracy_score  # This is correct\n",
    "\n",
    "\n",
    "print(\"Starting GridSearchCV for ORB Pipeline...\")\n",
    "grid_search_ORB.fit(train_df, train_df['Target'])\n",
    "print(\"GridSearchCV for ORB Pipeline completed.\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Best Parameters and Score for ORB\n",
    "# -----------------------------\n",
    "print(\"Best Parameters for ORB Pipeline:\", grid_search_ORB.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy for ORB Pipeline:\", grid_search_ORB.best_score_)\n",
    "\n",
    "# -----------------------------\n",
    "# Predict and Evaluate ORB Pipeline\n",
    "# -----------------------------\n",
    "best_ORB = grid_search_ORB.best_estimator_\n",
    "predictions_ORB = best_ORB.predict(test_df['histogram_ORB_IDF'])\n",
    "\n",
    "print(\"\\nORB SVM Classification Report:\")\n",
    "print(classification_report(test_df['Target'], predictions_ORB))\n",
    "print(\"ORB SVM Accuracy:\", accuracy_score(test_df['Target'], predictions_ORB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_ORB.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
